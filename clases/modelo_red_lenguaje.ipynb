{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eb00ebc7",
      "metadata": {
        "id": "eb00ebc7"
      },
      "outputs": [],
      "source": [
        "# Tools\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "from argparse import Namespace\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import FreqDist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "601090d2",
      "metadata": {
        "id": "601090d2"
      },
      "outputs": [],
      "source": [
        "seed = 1111\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83b0ae23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b0ae23",
        "outputId": "c5f6c0aa-4275-4c49-9709-4a82488ee796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N√∫mero de ejemplos de entrenamiento: 5278\n",
            "['@USUARIO @USUARIO @USUARIO Q se puede esperar del maricon de closet de la Ya√±ez aun recuerdo esa ves q lo vi en zona rosa viendo quien lo levantada', '@USUARIO La piel nueva siempre arde un poquito los primeros d√≠as... y m√°s con este puto clima', 'Ustedes no se enamoran de m√≠‚Ä¶ por tontas.', 'Me las va a pagar esa puta gorda roba tuits...', '@USUARIO LA GENTE ES TONTA PORQUE NO SE DAN CUENTA QUE T√ö HACES A BATMAN AZUL', 'Estoy muy encabronada con las pseudo feministas por tontas e iletradas, a veces me averg√ºenza ser mujer; preferir√≠a tener un falo. #NiUnaMas', 'Anden putos, recuerdan el #noerapenal #Holanda fuera de #Rusia2018, esto se llama #karma ehhhhhhhh #puuuuuutos', 'Si no tienen chichis no traten de ense√±ar se ven muy mal y m√°s cuando son prietas.', 'Ojal√° asi me agarrars cuando te digo que me voy en lugar de correrme a la verga cada 5 minutos.', '@USUARIO @USUARIO @USUARIO @USUARIO Es solo un HDP aprovechado y que su \"Diosito Bimbo\" me perdone']\n",
            "N√∫mero de ejemplos de validaci√≥n: 587\n",
            "['Al perro que se te acerque le parto su madre a si de facil', '@USUARIO @USUARIO √âl supo sacar a su familia adelante en lo que sabe en el mundo existen muchas personas ardidas como t√∫ ü§∑üèª\\u200d‚ôÄÔ∏è', '@USUARIO Entonces para que quieres estar en sus paises?, mejor vente aca y chinguele cabrona de verdad, maldita sangana.', 'Que bueno que hoy juega M√©xico, porque t√∫ vales verga, hija de la chingada.', 'Ojal√° un d√≠a me valgas la misma verga que al vato de la cfe despelucando y ponchando a medio del puente del puente a Cd judicial', 'Hoy mi outfit fue de licenciado/ pap√°... Padre que no vale verga y licenciado sin trabajo.', 'Me pregunto donde Chucha esta el supertanker   de la loca @USUARIO apagando estos incendios', 'Por un mega-error escuch√© parte de una canci√≥n de banda y decia \"Demosle vuelo a la hilacha\" Que putas madres üòÇüòÇ #PenaAjena', 'En M√©xico estaremos de la verga pero al menos no estamos de la verga con Trump dirigiendo al pa√≠s.', 'Nomames al principio pense que cancer de pacientes con cancer, ya les iba a mentar la madrea no seas asi']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "base = Path.cwd().parent\n",
        "x_train = pd.read_csv(base / 'Datos y Scripts-20250828' / \"mex20_train.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
        "x_val = pd.read_csv(base / 'Datos y Scripts-20250828' / \"mex20_val.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
        "print(\"N√∫mero de ejemplos de entrenamiento:\", len(x_train))\n",
        "print(x_train[:10])\n",
        "print(\"N√∫mero de ejemplos de validaci√≥n:\", len(x_val))\n",
        "print(x_val[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b718999",
      "metadata": {
        "id": "7b718999"
      },
      "outputs": [],
      "source": [
        "args = Namespace()\n",
        "args.N = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "833f5552",
      "metadata": {
        "id": "833f5552"
      },
      "outputs": [],
      "source": [
        "lista_excluidas = set(['.', ',', ';', ':', '!', '?', '¬ø', '¬°', '\"', \"'\", '(', ')', '[', ']', '{', '}', '-', '_', '‚Äî', '...',\n",
        "                       '@', '#', '$', '%', '^', '&', '*', '/', '|', '~', '`', '<', '>', '¬´', '¬ª', '‚Äú', '‚Äù', '‚Äò', '‚Äô','<url>','<@usuario>',\n",
        "\n",
        "                       ])\n",
        "\n",
        "class NgramData:\n",
        "    def __init__(self, N: int, vocab_size: int, tokenizer = None, embeddinds_model = None):\n",
        "        self.tokenizer = tokenizer if tokenizer is not None else self.default_tokenizer\n",
        "        self.punct = lista_excluidas\n",
        "        self.N = N\n",
        "        self.vocab_size = vocab_size\n",
        "        self.UNK = \"<unk>\"\n",
        "        self.SOS = \"<s>\"\n",
        "        self.EOS = \"</s>\"\n",
        "        self.embeddinds_model = embeddinds_model # TODO: implementar\n",
        "\n",
        "\n",
        "    def default_tokenizer(self, text: str) -> list:\n",
        "        return text.split()\n",
        "\n",
        "\n",
        "    def remove_word(self, word: str) -> bool:\n",
        "        word = word.lower()\n",
        "        is_punct = word in self.punct\n",
        "        is_digit = word.isdigit()\n",
        "        return is_punct or is_digit\n",
        "\n",
        "\n",
        "    def get_vocab(self, corpus: list) -> set:\n",
        "        freq_dist = FreqDist()\n",
        "        for sentence in corpus:\n",
        "            tokens = self.tokenizer(sentence)\n",
        "            tokens = [token.lower() for token in tokens if not self.remove_word(token)]\n",
        "            freq_dist.update(tokens)\n",
        "        most_common = freq_dist.most_common(self.vocab_size - 3)\n",
        "        vocab = set([word for word, _ in most_common])\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def fit(self, corpus: list) -> None:\n",
        "        self.vocab = self.get_vocab(corpus)\n",
        "        self.vocab.add(self.UNK)\n",
        "        self.vocab.add(self.SOS)\n",
        "        self.vocab.add(self.EOS)\n",
        "\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "\n",
        "        if self.embeddinds_model is not None:\n",
        "            self.embedding_matriz = np.empty([len(self.vocab), self.embeddinds_model.vector_size])\n",
        "\n",
        "        id = 0\n",
        "        for doc in corpus:\n",
        "            for word in self.tokenizer(doc):\n",
        "                word_ = word.lower()\n",
        "                if word_ in self.vocab and  word_ not in self.word_to_id:\n",
        "                    self.word_to_id[word_] = id\n",
        "                    self.id_to_word[id] = word_\n",
        "\n",
        "                    if self.embeddinds_model is not None:\n",
        "                        if word_ in self.embeddinds_model:\n",
        "                            self.embedding_matriz[id] = self.embeddinds_model[word_]\n",
        "                        else:\n",
        "                            self.embedding_matriz[id] = np.random.normal(self.embeddinds_model.vector_size)\n",
        "                    id += 1\n",
        "\n",
        "        self.word_to_id.update({self.UNK: id, self.SOS: id + 1, self.EOS: id + 2})\n",
        "        self.id_to_word.update({id: self.UNK, id + 1: self.SOS, id + 2: self.EOS})\n",
        "\n",
        "\n",
        "    def replace_unk(self, doc_tokens: list) -> list:\n",
        "        for i, token in enumerate(doc_tokens):\n",
        "            if token.lower() not in self.vocab:\n",
        "                doc_tokens[i] = self.UNK\n",
        "        return doc_tokens\n",
        "\n",
        "\n",
        "    def get_ngram_doc(self, doc: str) -> list:\n",
        "        doc_tokens = self.tokenizer(doc)\n",
        "        doc_tokens = self.replace_unk(doc_tokens)\n",
        "        doc_tokens = [w.lower() for w in doc_tokens]\n",
        "        doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
        "        return list(ngrams(doc_tokens, self.N))\n",
        "\n",
        "\n",
        "    def transform(self, corpus: list) -> tuple[np.ndarray, np.ndarray]:\n",
        "        x_ngrams = []\n",
        "        y = []\n",
        "\n",
        "        for doc in corpus:\n",
        "            doc_ngram = self.get_ngram_doc(doc)\n",
        "            for words_in_window in doc_ngram:\n",
        "                words_in_window_ids = [self.word_to_id[w] for w in words_in_window]\n",
        "                x_ngrams.append(list(words_in_window_ids[:-1]))\n",
        "                y.append(words_in_window_ids[-1])\n",
        "\n",
        "        return np.array(x_ngrams), np.array(y)\n",
        "\n",
        "\n",
        "    # =========== PROPOEDADES ===========\n",
        "    @property\n",
        "    def size(self) -> int:\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b7cee7f9",
      "metadata": {
        "id": "b7cee7f9"
      },
      "outputs": [],
      "source": [
        "tk = TweetTokenizer()\n",
        "\n",
        "ngram_data = NgramData(args.N, 5_000, tokenizer=tk.tokenize)\n",
        "ngram_data.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "15816d98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15816d98",
        "outputId": "d54d89e2-842d-41ce-c232-f696d9291571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tama√±o del vocabulario: 5,000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tama√±o del vocabulario: {ngram_data.size:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "703ead21",
      "metadata": {
        "id": "703ead21"
      },
      "outputs": [],
      "source": [
        "x_ngram_train, y_ngram_train = ngram_data.transform(x_train)\n",
        "x_ngram_val, y_ngram_val = ngram_data.transform(x_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "99f10d82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99f10d82",
        "outputId": "33877dbf-3e89-423c-9c05-e12b8cd0d0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4998, 4998, 4998],\n",
              "       [4998, 4998,    0],\n",
              "       [4998,    0,    0],\n",
              "       ...,\n",
              "       [4997,  937,   32],\n",
              "       [ 937,   32, 2524],\n",
              "       [  32, 2524, 4997]], shape=(102751, 3))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_ngram_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39281614",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39281614",
        "outputId": "1f86f0ec-7ed6-40d8-cef2-5c737293b896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ..., 2524, 4997, 4999], shape=(102751,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_ngram_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aea0e37c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aea0e37c",
        "outputId": "fb18fcce-c02a-4302-9517-287e6df749ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\n",
            "x_ngram_train: (102751, 3)\n",
            "y_ngram_train: (102751,)\n",
            "TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\n",
            "x_ngram_val: (11558, 3)\n",
            "y_ngram_val: (11558,)\n"
          ]
        }
      ],
      "source": [
        "# Tama√±os de los ngrams\n",
        "x_train_shape = x_ngram_train.shape\n",
        "y_train_shape = y_ngram_train.shape\n",
        "\n",
        "x_val_shape = x_ngram_val.shape\n",
        "y_val_shape = y_ngram_val.shape\n",
        "print(\"TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\")\n",
        "print(f\"x_ngram_train: {x_train_shape}\")\n",
        "print(f\"y_ngram_train: {y_train_shape}\")\n",
        "print(\"TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\")\n",
        "print(f\"x_ngram_val: {x_val_shape}\")\n",
        "print(f\"y_ngram_val: {y_val_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d8303ad4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8303ad4",
        "outputId": "b269c98d-ff48-4676-997c-a25a902c1763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: ['<s>', '<s>', '<s>']\n",
            "2: ['<s>', '<s>', '@usuario']\n",
            "3: ['<s>', '@usuario', '@usuario']\n",
            "4: ['@usuario', '@usuario', '@usuario']\n",
            "5: ['@usuario', '@usuario', 'q']\n",
            "6: ['@usuario', 'q', 'se']\n",
            "7: ['q', 'se', 'puede']\n",
            "8: ['se', 'puede', 'esperar']\n",
            "9: ['puede', 'esperar', 'del']\n",
            "10: ['esperar', 'del', 'maricon']\n",
            "11: ['del', 'maricon', 'de']\n",
            "12: ['maricon', 'de', 'closet']\n",
            "13: ['de', 'closet', 'de']\n",
            "14: ['closet', 'de', 'la']\n",
            "15: ['de', 'la', 'ya√±ez']\n",
            "16: ['la', 'ya√±ez', 'aun']\n",
            "17: ['ya√±ez', 'aun', 'recuerdo']\n",
            "18: ['aun', 'recuerdo', 'esa']\n",
            "19: ['recuerdo', 'esa', 'ves']\n",
            "20: ['esa', 'ves', 'q']\n",
            "21: ['ves', 'q', 'lo']\n",
            "22: ['q', 'lo', 'vi']\n"
          ]
        }
      ],
      "source": [
        "lista_palabras = [[ngram_data.id_to_word[w]  for w in tw] for tw in x_ngram_train[:22]]\n",
        "for i, palabras in enumerate(lista_palabras):\n",
        "    print(f\"{i+1}: {palabras}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "40142756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40142756",
        "outputId": "3b01f28b-2e7a-4380-979c-d0ea73355ad8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0, ..., 2524, 4997, 4999], shape=(102751,))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_ngram_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ed657786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed657786",
        "outputId": "89914d5f-57de-42e8-cd2e-19bd3d6b7cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: @usuario\n",
            "2: @usuario\n",
            "3: @usuario\n",
            "4: q\n",
            "5: se\n",
            "6: puede\n",
            "7: esperar\n",
            "8: del\n",
            "9: maricon\n",
            "10: de\n",
            "11: closet\n",
            "12: de\n",
            "13: la\n",
            "14: ya√±ez\n",
            "15: aun\n",
            "16: recuerdo\n",
            "17: esa\n",
            "18: ves\n",
            "19: q\n",
            "20: lo\n",
            "21: vi\n",
            "22: en\n"
          ]
        }
      ],
      "source": [
        "lista_palbras_en_sus_ys = [ngram_data.id_to_word[w] for w in y_ngram_train[:22]]\n",
        "for i, palabra in enumerate(lista_palbras_en_sus_ys):\n",
        "    print(f\"{i+1}: {palabra}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "82224a75",
      "metadata": {
        "id": "82224a75"
      },
      "outputs": [],
      "source": [
        "args.batch_size = 64\n",
        "args.num_workers = 0\n",
        "DTYPE = torch.int64\n",
        "\n",
        "def tensor_dataset(x: np.ndarray, y: np.ndarray) -> TensorDataset:\n",
        "    tensor_data = TensorDataset(\n",
        "        torch.tensor(x, dtype=DTYPE),\n",
        "        torch.tensor(y, dtype=DTYPE))\n",
        "    return tensor_data\n",
        "\n",
        "def data_loader(dataset: TensorDataset, shuffle: bool) -> DataLoader:\n",
        "    dataloader = DataLoader(dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=shuffle)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# Crear los DataLoaders\n",
        "train_dataset = tensor_dataset(x_ngram_train, y_ngram_train)\n",
        "\n",
        "train_loader = data_loader(train_dataset, shuffle=True)\n",
        "\n",
        "val_dataset = tensor_dataset(x_ngram_val, y_ngram_val)\n",
        "\n",
        "val_loader = data_loader(val_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aghsPDCT8XJF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aghsPDCT8XJF",
        "outputId": "8a889b61-1283-44f3-edcf-82fa283eddef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: torch.Size([64, 3])\n",
            "Y shape: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(f'X shape: {batch[0].shape}')\n",
        "print(f'Y shape: {batch[1].shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cbjoHNfV-vOF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbjoHNfV-vOF",
        "outputId": "9ab1c2ff-86c5-4467-f4fe-58b4ba720276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4998,    0,  235],\n",
              "        [  32,   27, 4997],\n",
              "        [   7,  218,  161],\n",
              "        [ 109,    7, 1139],\n",
              "        [4998, 4998, 4998],\n",
              "        [  48,  342,   45],\n",
              "        [  43,    9, 1325],\n",
              "        [4997, 4997,    7],\n",
              "        [4997, 1799,  225],\n",
              "        [4998, 4997,   48],\n",
              "        [   7,    9,  565],\n",
              "        [4998,  700, 4997],\n",
              "        [4997, 4997, 4997],\n",
              "        [3538, 4997,  165],\n",
              "        [4998, 4998, 4998],\n",
              "        [  15, 4997, 4997],\n",
              "        [ 114,    2, 3261],\n",
              "        [ 455,   51,  456],\n",
              "        [  45, 4997,   83],\n",
              "        [  48,  375,   48],\n",
              "        [ 942,   43, 2500],\n",
              "        [  60,   45, 2302],\n",
              "        [ 253,   48,  255],\n",
              "        [  55,   48,   21],\n",
              "        [ 419, 2462,   33],\n",
              "        [4998, 4998, 4998],\n",
              "        [4997, 4997,   66],\n",
              "        [ 931, 4997, 4997],\n",
              "        [  48,  166,  128],\n",
              "        [1595,  337,   17],\n",
              "        [ 114,   48,   46],\n",
              "        [ 980, 4997,  696],\n",
              "        [4998, 4998, 4998],\n",
              "        [ 112,  272, 4997],\n",
              "        [1559,  192,  705],\n",
              "        [ 106, 4997,   93],\n",
              "        [ 266,   17,  919],\n",
              "        [4998, 1250,   45],\n",
              "        [4997,   60, 4142],\n",
              "        [1595,  318, 2392],\n",
              "        [4997,   39, 4274],\n",
              "        [1228,   83, 4997],\n",
              "        [ 160, 4997,   34],\n",
              "        [4998,   48,    9],\n",
              "        [ 253,   27, 4467],\n",
              "        [4998, 4998,  865],\n",
              "        [  32,  124,  766],\n",
              "        [4997,   55,   27],\n",
              "        [  39, 2318, 1880],\n",
              "        [ 635,    0,   32],\n",
              "        [  65, 2204,    7],\n",
              "        [4998, 4998,   39],\n",
              "        [3176, 4997, 4997],\n",
              "        [   7,    9, 1264],\n",
              "        [4998, 4998, 4998],\n",
              "        [ 318,  501, 4997],\n",
              "        [2136,  128,  161],\n",
              "        [1579,   60, 1235],\n",
              "        [4998,  252, 4997],\n",
              "        [   7,  117,   50],\n",
              "        [ 128, 4997,   32],\n",
              "        [ 615,  615,   81],\n",
              "        [4997,  244,   60],\n",
              "        [4998,  420,  197]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eU6r7FaP_hBv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU6r7FaP_hBv",
        "outputId": "dc62adef-ff1e-4478-8b39-b3ad01727e85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['<s>', '@usuario', 'luchona'],\n",
              " ['y', 'un', '<unk>'],\n",
              " ['de', 'perra', 'como'],\n",
              " ['lugar', 'de', 'unas'],\n",
              " ['<s>', '<s>', '<s>'],\n",
              " ['a', 'mi', 'me'],\n",
              " ['por', 'la', 'noche'],\n",
              " ['<unk>', '<unk>', 'de'],\n",
              " ['<unk>', 'instagram', 'para'],\n",
              " ['<s>', '<unk>', 'a'],\n",
              " ['de', 'la', 'palabra'],\n",
              " ['<s>', 'ah', '<unk>'],\n",
              " ['<unk>', '<unk>', '<unk>'],\n",
              " ['favoritas', '<unk>', 'pero'],\n",
              " ['<s>', '<s>', '<s>'],\n",
              " ['lo', '<unk>', '<unk>'],\n",
              " ['solo', 'se', 'escucha'],\n",
              " ['dise', 'gorda', 'nena'],\n",
              " ['me', '<unk>', 'el'],\n",
              " ['a', 'comprar', 'a'],\n",
              " ['triste', 'por', 'joe'],\n",
              " ['que', 'me', 'anda'],\n",
              " ['tiene', 'a', 'gusto'],\n",
              " ['es', 'a', 'quien'],\n",
              " ['hay', 'momento', 'm√°s'],\n",
              " ['<s>', '<s>', '<s>'],\n",
              " ['<unk>', '<unk>', 'muy'],\n",
              " ['jugar', '<unk>', '<unk>'],\n",
              " ['a', 'tu', 'madre'],\n",
              " ['a√∫n', 'sigo', 'en'],\n",
              " ['solo', 'a', 'las'],\n",
              " ['soluci√≥n', '<unk>', 'luego'],\n",
              " ['<s>', '<s>', '<s>'],\n",
              " ['cada', 'foto', '<unk>'],\n",
              " ['mamadas', '..', 'eso'],\n",
              " ['te', '<unk>', 'si'],\n",
              " ['hoy', 'en', 'd√≠a'],\n",
              " ['<s>', 'uuuugh', 'me'],\n",
              " ['<unk>', 'que', 'regresen'],\n",
              " ['a√∫n', 'as√≠', 'encuentro'],\n",
              " ['<unk>', 'no', 'fumo'],\n",
              " ['llegar', 'el', '<unk>'],\n",
              " ['putas', '<unk>', 'con'],\n",
              " ['<s>', 'a', 'la'],\n",
              " ['tiene', 'un', 'genio'],\n",
              " ['<s>', '<s>', 'ver'],\n",
              " ['y', 'est√°', 'bien'],\n",
              " ['<unk>', 'es', 'un'],\n",
              " ['no', 't', 'sacan'],\n",
              " ['üòÇ', '@usuario', 'y'],\n",
              " ['estoy', 'loco', 'de'],\n",
              " ['<s>', '<s>', 'no'],\n",
              " ['largo', '<unk>', '<unk>'],\n",
              " ['de', 'la', 'selecci√≥n'],\n",
              " ['<s>', '<s>', '<s>'],\n",
              " ['as√≠', 'dice', '<unk>'],\n",
              " ['valgo', 'madre', 'como'],\n",
              " ['quieren', 'que', 'uno'],\n",
              " ['<s>', 'ni', '<unk>'],\n",
              " ['de', 'su', 'puta'],\n",
              " ['madre', '<unk>', 'y'],\n",
              " ['m√©xico', 'm√©xico', 'putos'],\n",
              " ['<unk>', 'ni√±o', 'que'],\n",
              " ['<s>', 'cosas', 'feas']]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[[ngram_data.id_to_word[w] for w in tw] for tw in batch[0].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "Me3_xZk9_9ye",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me3_xZk9_9ye",
        "outputId": "aad58227-4121-4185-ffd8-239b80e5a2a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  32, 4997,   15, 4997,    7, 3093, 1304,  575,  160, 2407, 4999,  165,\n",
              "        3840,  955,   35, 4999,   60,  242, 1207,    9, 3119, 3050, 4997,   33,\n",
              "        4997,    0, 4045,   45,  166, 4997,   33,   57,   83,   55,   39,  106,\n",
              "         101,   65, 4997,  109, 4997,  100,   46,  111,    7,   48, 4997, 1916,\n",
              "          34,    0, 4997,  419,   48, 4997,    0, 4997,   93,   70, 4997,  128,\n",
              "         318,  385,  923,   32])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "gxwIelqmCDOx",
      "metadata": {
        "id": "gxwIelqmCDOx"
      },
      "outputs": [],
      "source": [
        "class NeuralLM(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(NeuralLM, self).__init__()\n",
        "        self.window_size = args.N - 1\n",
        "        self.embedding_size = args.d\n",
        "\n",
        "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
        "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d)\n",
        "        self.drop1 = nn.Dropout(p=args.dropout)\n",
        "        self.fc2 = nn.Linear(args.d, args.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(-1, self.embedding_size * self.window_size)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = self.drop1(h)\n",
        "        return self.fc2(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "Ie2MDWZuGXFL",
      "metadata": {
        "id": "Ie2MDWZuGXFL"
      },
      "outputs": [],
      "source": [
        "def get_preds(raw_logits):\n",
        "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
        "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "kmOIbjSgI8M1",
      "metadata": {
        "id": "kmOIbjSgI8M1"
      },
      "outputs": [],
      "source": [
        "def model_eval(data, model, gpu=False):\n",
        "    with torch.no_grad():\n",
        "        preds, tgts = [], []\n",
        "        for window_words, labels in data:\n",
        "            if gpu:\n",
        "                window_words = window_words.cuda()\n",
        "\n",
        "            outputs = model(window_words)\n",
        "            y_pred = get_preds(outputs)\n",
        "\n",
        "            tgt = labels.numpy()\n",
        "            tgts.append(tgt)\n",
        "            preds.append(y_pred)\n",
        "\n",
        "    tgts = [e for l in tgts for e in l]\n",
        "    preds = [e for l in preds for e in l]\n",
        "\n",
        "    return accuracy_score(tgts, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "FLdgl3hpI_h_",
      "metadata": {
        "id": "FLdgl3hpI_h_"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
        "    filename = os.path.join(checkpoint_path, filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "v0yaw6BnJBhq",
      "metadata": {
        "id": "v0yaw6BnJBhq"
      },
      "outputs": [],
      "source": [
        "# Model hyparameters\n",
        "args.vocab_size = ngram_data.size\n",
        "args.d = 100\n",
        "args.d_h = 200\n",
        "args.dropout = 0.1\n",
        "\n",
        "# Training hyperparameters\n",
        "args.lr = 2.3e-1\n",
        "args.num_epochs = 100\n",
        "args.patience = 20\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "args.lr_patience = 10\n",
        "args.lr_factor = 0.5\n",
        "\n",
        "# Saving directory\n",
        "args.savedir = 'model'\n",
        "os.makedirs(args.savedir, exist_ok=True)\n",
        "\n",
        "# Create model\n",
        "model = NeuralLM(args)\n",
        "\n",
        "# Send to GPU\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "if args.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "# Loss, Optimizer and Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    patience=args.lr_patience,\n",
        "    factor=args.lr_factor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1PsiC6SaLuwu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PsiC6SaLuwu",
        "outputId": "4686a944-0d8b-4336-d790-b988f77dc8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc 0.1774\n",
            "Epoch: 0, Loss: 5.5258, Val acc: 0.1738\n",
            "Epoch time: 6.8022\n",
            "Train acc 0.1844\n",
            "Epoch: 1, Loss: 5.1077, Val acc: 0.2257\n",
            "Epoch time: 6.1975\n",
            "Train acc 0.1885\n",
            "Epoch: 2, Loss: 4.9256, Val acc: 0.2016\n",
            "Epoch time: 5.9357\n",
            "Train acc 0.1923\n",
            "Epoch: 3, Loss: 4.7968, Val acc: 0.2252\n",
            "Epoch time: 9.1137\n",
            "Train acc 0.1925\n",
            "Epoch: 4, Loss: 4.6915, Val acc: 0.2022\n",
            "Epoch time: 5.4584\n",
            "Train acc 0.1970\n",
            "Epoch: 5, Loss: 4.5930, Val acc: 0.2291\n",
            "Epoch time: 6.2871\n",
            "Train acc 0.1952\n",
            "Epoch: 6, Loss: 4.5077, Val acc: 0.2278\n",
            "Epoch time: 6.6156\n",
            "Train acc 0.1937\n",
            "Epoch: 7, Loss: 4.4343, Val acc: 0.2313\n",
            "Epoch time: 6.2607\n",
            "Train acc 0.1965\n",
            "Epoch: 8, Loss: 4.3639, Val acc: 0.1871\n",
            "Epoch time: 6.9253\n",
            "Train acc 0.1953\n",
            "Epoch: 9, Loss: 4.3018, Val acc: 0.1678\n",
            "Epoch time: 7.5081\n",
            "Train acc 0.1954\n",
            "Epoch: 10, Loss: 4.2397, Val acc: 0.2131\n",
            "Epoch time: 7.7477\n",
            "Train acc 0.1986\n",
            "Epoch: 11, Loss: 4.1808, Val acc: 0.1678\n",
            "Epoch time: 6.8429\n",
            "Train acc 0.1981\n",
            "Epoch: 12, Loss: 4.1288, Val acc: 0.2141\n",
            "Epoch time: 7.3568\n",
            "Train acc 0.1998\n",
            "Epoch: 13, Loss: 4.0776, Val acc: 0.1198\n",
            "Epoch time: 8.7192\n",
            "Train acc 0.2017\n",
            "Epoch: 14, Loss: 4.0343, Val acc: 0.1953\n",
            "Epoch time: 7.2184\n",
            "Train acc 0.2030\n",
            "Epoch: 15, Loss: 3.9858, Val acc: 0.2019\n",
            "Epoch time: 6.8602\n",
            "Train acc 0.2065\n",
            "Epoch: 16, Loss: 3.9448, Val acc: 0.1875\n",
            "Epoch time: 6.3117\n",
            "Train acc 0.2103\n",
            "Epoch: 17, Loss: 3.9040, Val acc: 0.2209\n",
            "Epoch time: 6.6003\n",
            "Train acc 0.2127\n",
            "Epoch: 18, Loss: 3.8646, Val acc: 0.1991\n",
            "Epoch time: 6.8518\n",
            "Train acc 0.2143\n",
            "Epoch: 19, Loss: 3.8297, Val acc: 0.2121\n",
            "Epoch time: 6.7240\n",
            "Train acc 0.2196\n",
            "Epoch: 20, Loss: 3.7954, Val acc: 0.1714\n",
            "Epoch time: 7.1512\n",
            "Train acc 0.2196\n",
            "Epoch: 21, Loss: 3.7693, Val acc: 0.2155\n",
            "Epoch time: 7.3708\n",
            "Train acc 0.2248\n",
            "Epoch: 22, Loss: 3.7359, Val acc: 0.2144\n",
            "Epoch time: 6.8784\n",
            "Train acc 0.2298\n",
            "Epoch: 23, Loss: 3.7061, Val acc: 0.1958\n",
            "Epoch time: 6.6247\n",
            "Train acc 0.2306\n",
            "Epoch: 24, Loss: 3.6848, Val acc: 0.2231\n",
            "Epoch time: 6.4818\n",
            "Train acc 0.2666\n",
            "Epoch: 25, Loss: 3.4415, Val acc: 0.2243\n",
            "Epoch time: 6.8544\n",
            "Train acc 0.2694\n",
            "Epoch: 26, Loss: 3.4049, Val acc: 0.1734\n",
            "Epoch time: 6.6305\n",
            "No improvement. Breaking out of loop.\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(args.num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    loss_epoch = []\n",
        "    training_metric = []\n",
        "    model.train()\n",
        "\n",
        "    for window_words, labels in train_loader:\n",
        "\n",
        "        # If GPU available\n",
        "        if args.use_gpu:\n",
        "            window_words = window_words.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(window_words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_epoch.append(loss.item())\n",
        "\n",
        "        # Get training metrics\n",
        "        y_pred = get_preds(outputs)\n",
        "        tgt = labels.cpu().numpy()\n",
        "        training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    mean_epoch_metric = np.mean(training_metric)\n",
        "    train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "    # Get metric in validation dataset\n",
        "    model.eval()\n",
        "    tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
        "    metric_history.append(tuning_metric)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(tuning_metric)\n",
        "\n",
        "    # Check for metric improvement\n",
        "    is_improvement = tuning_metric > best_metric\n",
        "    if is_improvement:\n",
        "        best_metric = tuning_metric\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    # Save best model if metric improved\n",
        "    save_checkpoint(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'best_metric': best_metric,\n",
        "        },\n",
        "        is_improvement,\n",
        "        args.savedir,\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    if no_improve >= args.patience:\n",
        "        print(\"No improvement. Breaking out of loop.\")\n",
        "        break\n",
        "\n",
        "    print(f'Train acc {mean_epoch_metric:.4f}')\n",
        "    print(f'Epoch: {epoch}, Loss: {np.mean(loss_epoch):.4f}, Val acc: {tuning_metric:.4f}')\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f'Epoch time: {epoch_time:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "MBjNqAkcNFz5",
      "metadata": {
        "id": "MBjNqAkcNFz5"
      },
      "outputs": [],
      "source": [
        "def print_closest_words(embeddings, ngram_data, word, n):\n",
        "    word_id = torch.LongTensor([ngram_data.word_to_id[word]])  # get word id\n",
        "    word_embed = embeddings(word_id)                     # get word embedding\n",
        "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()  # compute distances to all words\n",
        "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1])          # sort by distance\n",
        "    for idx, difference in lst[1:n+1]:                                 # take the top n, ignore word itself\n",
        "        print(ngram_data.id_to_word[idx], difference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4lG63TehUVI9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lG63TehUVI9",
        "outputId": "874305cc-beb5-45ff-cac8-7b8970821679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Learned embeddings\n",
            "------------------------------\n",
            "<unk> 10.375457\n",
            "dejarte 10.661759\n",
            "laferte 10.887369\n",
            "mueven 10.964158\n",
            "pared 10.99325\n",
            "muchas 11.043353\n",
            "tenia 11.092116\n",
            "vaso 11.1542015\n",
            "haciendose 11.167336\n",
            "pregunt√≥ 11.192324\n"
          ]
        }
      ],
      "source": [
        "# Model with learned embeddings from scratch\n",
        "best_model = NeuralLM(args)\n",
        "best_model.load_state_dict(torch.load('model/model_best.pt')['state_dict'])\n",
        "best_model.train(False)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Learned embeddings\")\n",
        "print(\"-\" * 30)\n",
        "print_closest_words(best_model.emb, ngram_data, \"chivas\", 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "GQxwPBPLUV--",
      "metadata": {
        "id": "GQxwPBPLUV--"
      },
      "outputs": [],
      "source": [
        "def parse_text(text, tokenizer):\n",
        "  all_tokens = [w.lower() if w in ngram_data.word_to_id else '<unk>' for w in tokenizer.tokenize(text)]\n",
        "  tokens_id = [ngram_data.word_to_id[word.lower()] for word in all_tokens]\n",
        "  return all_tokens, tokens_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "-dqx9hgetfrZ",
      "metadata": {
        "id": "-dqx9hgetfrZ"
      },
      "outputs": [],
      "source": [
        "def sample_next_word(logits, temperature=1.0):\n",
        "    logits = np.asarray(logits).astype('float64')\n",
        "    preds = logits / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "JigR1OlBu7E6",
      "metadata": {
        "id": "JigR1OlBu7E6"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(model, token_ids):\n",
        "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
        "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
        "\n",
        "    # y_probs = F.softmax(y_raw_pred, dim=1)\n",
        "    # y_pred = torch.argmax(y_probs, dim=1).detach().numpy()\n",
        "\n",
        "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "CkGu-pO8u-KM",
      "metadata": {
        "id": "CkGu-pO8u-KM"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, initial_text, tokenizer):\n",
        "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
        "\n",
        "    for i in range(100):\n",
        "        y_pred = predict_next_token(model, window_word_ids)\n",
        "        next_word = ngram_data.id_to_word[y_pred]\n",
        "        all_tokens.append(next_word)\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "        else:\n",
        "            window_word_ids.pop(0)\n",
        "            window_word_ids.append(y_pred)\n",
        "\n",
        "    return \" \".join(all_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "Ju2tUmj8vAds",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju2tUmj8vAds",
        "outputId": "67b30b9e-d67d-4265-dd8e-00b01dff3e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Learned embeddings\n",
            "------------------------------\n",
            "<s> las chivas bien dinero <unk> como estaba ver un <unk> üò© üò© </s>\n"
          ]
        }
      ],
      "source": [
        "initial_tokens = '<s> las chivas'\n",
        "\n",
        "print('-' * 30)\n",
        "print('Learned embeddings')\n",
        "print('-' * 30)\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Wk6xJwpSvjlN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk6xJwpSvjlN",
        "outputId": "63f3f5f1-a939-4853-db36-db7027bcd7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Learned embeddings\n",
            "------------------------------\n",
            "yo opino que <unk> tecatito letra puto <unk> anda a gustar brayan t√≠a una los <unk> esos de sexo <unk> hasta es que me hago <unk> <unk> <unk> üçÜ üò¢ o tenemos <unk> es la masa que ya <unk> par de ser <unk> <unk> a todos cu√°ntas <unk> su culera <unk> <unk> pero igual hdp üé∂ </s>\n"
          ]
        }
      ],
      "source": [
        "initial_tokens = 'yo opino que'\n",
        "\n",
        "print('-' * 30)\n",
        "print('Learned embeddings')\n",
        "print('-' * 30)\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "FgHQlB3NxaQy",
      "metadata": {
        "id": "FgHQlB3NxaQy"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(model, text, ngram_data):\n",
        "    # Generate n-gram windows from input text and the respective label y\n",
        "    X, y = ngram_data.transform([text])\n",
        "    # Discard first two n-gram windows since they contain <s> tokens not necessary\n",
        "    X, y = X[2:], y[2:]\n",
        "    X = torch.LongTensor(X).unsqueeze(0)\n",
        "\n",
        "    logits = model(X).detach()\n",
        "    probs = F.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "_BXvDMfiyjdy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BXvDMfiyjdy",
        "outputId": "395a51a3-94d2-4f9f-9f42-833dce67396f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log likelihood: -32.4111\n"
          ]
        }
      ],
      "source": [
        "print(\"log likelihood:\", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje natural\", ngram_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "crpxhGMCyluO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crpxhGMCyluO",
        "outputId": "00691ae8-fe07-46d3-84e7-d3a64897ee8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log likelihood: -41.06167\n"
          ]
        }
      ],
      "source": [
        "print(\"log likelihood:\", log_likelihood(best_model, \"Estamos procesamiento clase en la de natural de lenguaje\", ngram_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25kb-dYlzDP2",
      "metadata": {
        "id": "25kb-dYlzDP2"
      },
      "source": [
        "# Estructuras Sintacticas correctas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "5Ui5P3ydynjB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ui5P3ydynjB",
        "outputId": "da2c164e-cd34-4356-e6b0-e09b1824281d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-21.7664 sino gano a la chingada me voy\n",
            "-22.4687 gano sino a la chingada me voy\n",
            "-22.5653 gano sino me voy a la chingada\n",
            "-23.4178 sino gano me voy a la chingada\n",
            "-24.7224 gano sino la chingada me voy a\n",
            "--------------------------------------------------\n",
            "-63.4815 la a voy sino chingada gano me\n",
            "-63.8410 me a voy chingada gano sino la\n",
            "-63.9567 a la sino voy chingada gano me\n",
            "-64.6745 a me chingada voy sino gano la\n",
            "-64.9590 la me a voy chingada gano sino\n"
          ]
        }
      ],
      "source": [
        "from itertools import permutations\n",
        "from random import shuffle\n",
        "\n",
        "word_list = \"sino gano me voy a la chingada\".split(' ')\n",
        "perms = [' '.join(perm) for perm in permutations(word_list)]\n",
        "print('-' * 50)\n",
        "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
        "    print(f\"{p:.4f} {t}\")\n",
        "\n",
        "print('-' * 50)\n",
        "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
        "    print(f\"{p:.4f} {t}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lKATDnKf0QHf",
      "metadata": {
        "id": "lKATDnKf0QHf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
